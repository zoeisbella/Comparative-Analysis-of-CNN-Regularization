# ğŸ§  Comparative Analysis of CNN Regularization Techniques

> ğŸ” A Visual Experiment on How Data Augmentation, Group Normalization & Dropout Affect Model Performance

## ğŸ“– Project Background

In deep learning image classification tasks:
- ğŸ¨ **Data Augmentation** expands training data diversity
- ğŸ›‘ **Dropout** helps mitigate overfitting risks  
- âš–ï¸ **Group Normalization** stabilizes the training process

This project designs **8 controlled experiments** ğŸ§ª to quantitatively analyze how these techniques perform individually and in combination, providing practical insights for model optimization in similar tasks! ğŸ“Š

## ğŸ›  Tech Stack

| Component | Technology |
|-----------|------------|
| ğŸ§© **Deep Learning Framework** | TensorFlow 2.x + Keras |
| ğŸ—‚ **Dataset** | CIFAR-10 (10 classes of 32Ã—32 color images) |
| ğŸ“ˆ **Visualization Tool** | Matplotlib |
| ğŸ’» **Runtime Environment** | Python 3.7+, Jupyter Notebook (or any Python IDE) |

## ğŸ§ª Experimental Design

We designed **8 technical combinations** covering four dimensions:

| Dimension | Description | 
|-----------|-------------|
| ğŸ”° **Baseline Model** | Plain CNN without regularization |
| âš¡ **Single Technique** | One regularization method at a time |
| ğŸ¤ **Dual-Technique Combination** | Pairwise combinations |
| ğŸ¯ **Triple-Technique Combination** | All three techniques together |

## ğŸ“Š Experimental Results
### 1. ğŸ“ˆ Accuracy Curve Comparison

**Left**: Training Accuracy Curve 
- Shows model's learning progress on training data
- Indicates how well models "practice" on seen data ğŸ‹ï¸  
**Right**: Validation Accuracy Curve 
- Shows model's performance on unseen test data ğŸ“
- Reflects true generalization capability
<img width="1789" height="536" alt="image" src="https://github.com/user-attachments/assets/ffe4251e-91b2-4d74-96aa-9e2d42ba26dd" />

### 2. âœ… Final Test Accuracy

| Experiment Configuration | Test Accuracy |
|--------------------------|---------------|
| Baseline | 0.5787 |
| Augmentation | 0.5289 |
| Dropout | 0.5604 |
| Normalization | 0.6349 |
| Aug + Dropout | 0.4767 |
| Aug + Norm | 0.5745 |
| Dropout + Norm | 0.6072 |
| Aug + Dropout + Norm | 0.5039 |
## ğŸ” Key Findings 

**ğŸ¯ Normalization Delivers Best Results**  
Using only Group Normalization achieved the highest accuracy (0.6349) - approximately 5.6% improvement over baseline - demonstrating its crucial role in stabilizing training and enhancing model generalization.

**âš¡ Strategic Combinations Show Promise**  
The Dropout + Normalization combination (0.6072) performed excellently, indicating complementary effects between these regularization methods: normalization handles internal covariate shift while Dropout prevents feature co-adaptation.

**ğŸš« Data Augmentation & Dropout Conflict**  
The combination of Data Augmentation and Dropout yielded the poorest results (0.4767), suggesting that using both simultaneously may cause over-regularization and excessively limit model learning capacity.

**ğŸ“‰ Triple Combination Shows Diminishing Returns**  
Using all three techniques together (0.5039) underperformed, proving that the "more is better" approach doesn't apply to regularization combinations - balance is essential.

**ğŸ“Š Baseline Provides Critical Reference**  
The basic CNN model (0.5787) established a clear benchmark, confirming that any effective regularization combination must significantly exceed this performance level to be practically valuable.
---

ğŸš€ *Explore the code and see how these techniques transform model performance!* 
